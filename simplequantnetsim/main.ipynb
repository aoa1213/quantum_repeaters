{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80939b38-00a9-40db-9a16-2ac462295f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"networkx backend defined more than once\")\n",
    "import sys\n",
    "import os\n",
    "import gc\n",
    "import csv\n",
    "import copy\n",
    "import json\n",
    "import math\n",
    "import shutil\n",
    "import random\n",
    "import pickle\n",
    "import itertools\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # 非交互式后端\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import tqdm as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from protocols import MPC_protocol, MPG_protocol, SP_protocol\n",
    "from graph import network, set_p_edge\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from networkx.algorithms.community import greedy_modularity_communities\n",
    "from networkx.drawing.layout import *\n",
    "\n",
    "notebook_path = os.path.abspath(\"\")\n",
    "\n",
    "from config import DATA_PATHS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7218c88f-2273-4dc1-a27a-e12c43c87820",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkr = ['x','+','d','o','1','2']+['x']*100\n",
    "dashs = ['-.','--',':','-']+['-']*100\n",
    "cols = ['gray','g','b','orange','r','k','purple']+['k']*100\n",
    "linewidth = 2.2\n",
    "mks = 5.5\n",
    "fontsize = 14\n",
    "sys.path.append(\"..\")\n",
    "root_path = DATA_PATHS[\"input_graphs\"]\n",
    "LOOP_STATE_PATH = \"loop_state.pkl\"\n",
    "TEMP_DATA_PATH = \"temp_data.pkl\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0cd02129-51d1-4981-b3a3-3bd9837b9fc0",
   "metadata": {},
   "source": [
    "Find the ER for the MPC, MPG, and SP protocols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf39d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import networkx as nx\n",
    "\n",
    "def load_data(filepath):\n",
    "    \"\"\"\n",
    "    Load a network graph from a JSON file in node-link format.\n",
    "\n",
    "    The JSON file is expected to contain:\n",
    "      - \"nodes\": a list of nodes, each with fields \"id\", \"latitude\", \"longitude\", \"location\", and \"country\"\n",
    "      - \"links\": a list of edges, each with fields \"source\", \"target\", and \"length\"\n",
    "\n",
    "    This function:\n",
    "      - Builds a NetworkX graph with node and edge attributes\n",
    "      - Stores node positions using (longitude, latitude) format\n",
    "      - Collects the ID of the first node (assumed to be the fixed or user node)\n",
    "      - Prints the degree of the first node for verification\n",
    "\n",
    "    Args:\n",
    "        filepath (str or Path): Path to the input JSON file.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - G (networkx.Graph): The constructed graph.\n",
    "            - user (list): A list containing the first node's ID.\n",
    "            - pos (dict): Mapping from node ID to (longitude, latitude) positions.\n",
    "    \"\"\"\n",
    "    pos = {}\n",
    "    user = []\n",
    "\n",
    "    # Step 1: Read JSON file\n",
    "    with open(filepath, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Step 2: Initialize graph\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Step 3: Add nodes\n",
    "    for node in data[\"nodes\"]:\n",
    "        node_id = node[\"id\"]\n",
    "        x, y = node[\"latitude\"], node[\"longitude\"]\n",
    "        G.add_node(node_id, location=node[\"location\"], country=node[\"country\"])  # Add node to graph\n",
    "        pos[node_id] = (y, x)  # Store node position as (longitude, latitude)\n",
    "\n",
    "    # Step 4: Add edges\n",
    "    for edge in data[\"links\"]:\n",
    "        source = int(edge[\"source\"])\n",
    "        target = int(edge[\"target\"])\n",
    "        G.add_edge(source, target, length=edge[\"length\"])  # Add edge to graph\n",
    "\n",
    "    degree_dict = dict(G.degree())\n",
    "    degree_items = list(degree_dict.items())\n",
    "    first_node, first_degree = degree_items[0]\n",
    "    print(f\"First node ID: {first_node}, Degree: {first_degree}\")\n",
    "\n",
    "    user.append(data[\"nodes\"][0][\"id\"])\n",
    "\n",
    "    return G, user, pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29238b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_iterative_score_partition_with_drawing(\n",
    "    G, \n",
    "    fixed_node, \n",
    "    alpha=1.0, \n",
    "    beta=1.0,\n",
    "    max_rounds=10,\n",
    "    shuffle_nodes=True,\n",
    "    pos=None,\n",
    "    output_path=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Multi-round iterative score-based community partitioning with optional visualization:\n",
    "      - Number of communities = degree(fixed_node) + 1\n",
    "      - Initialization: fixed_node goes to community 0; each neighbor to a separate community\n",
    "      - Assignment: score = alpha * distance + beta * community_size\n",
    "      - Iteration: if moving reduces the score, move the node; repeat until stable or max_rounds\n",
    "      - Optionally draws and saves the graph with colored communities\n",
    "\n",
    "    Returns:\n",
    "        communities (list of sets): Final partitioned communities.\n",
    "        all_key_nodes_combos (list of lists): All combinations by picking one node from each non-zero community,\n",
    "                                              with fixed_node prepended to each.\n",
    "    \"\"\"\n",
    "    # ============ 1) Initialize community containers ============\n",
    "    neighbors = list(G.neighbors(fixed_node))\n",
    "    num_communities = len(neighbors)  # Number of communities excluding community 0\n",
    "    communities = [set() for _ in range(num_communities + 1)]\n",
    "    visited = set()\n",
    "\n",
    "    # Community 0 contains the fixed_node\n",
    "    communities[0].add(fixed_node)\n",
    "    visited.add(fixed_node)\n",
    "\n",
    "    # Each neighbor starts in its own community\n",
    "    for i, nb in enumerate(neighbors, start=1):\n",
    "        communities[i].add(nb)\n",
    "        visited.add(nb)\n",
    "\n",
    "    # ============ 2) Initial assignment of remaining nodes ============\n",
    "    for node in G.nodes():\n",
    "        if node not in visited:\n",
    "            best_score = float('inf')\n",
    "            best_index = None\n",
    "            for i, nb in enumerate(neighbors, start=1):\n",
    "                dist = nx.shortest_path_length(G, source=node, target=nb)\n",
    "                size = len(communities[i])\n",
    "                score = alpha * dist + beta * size\n",
    "                if score < best_score:\n",
    "                    best_score = score\n",
    "                    best_index = i\n",
    "            communities[best_index].add(node)\n",
    "            visited.add(node)\n",
    "\n",
    "    # ============ 3) Multi-round migration ============\n",
    "    round_num = 0\n",
    "    while round_num < max_rounds:\n",
    "        round_num += 1\n",
    "        moved_count = 0\n",
    "\n",
    "        # All nodes except the fixed_node\n",
    "        all_nodes = [n for n in G.nodes() if n != fixed_node]\n",
    "\n",
    "        if shuffle_nodes:\n",
    "            random.shuffle(all_nodes)\n",
    "\n",
    "        for node in all_nodes:\n",
    "            current_idx = None\n",
    "            for i, comm in enumerate(communities):\n",
    "                if node in comm:\n",
    "                    current_idx = i\n",
    "                    break\n",
    "\n",
    "            # Skip the fixed_node\n",
    "            if current_idx == 0:\n",
    "                continue\n",
    "\n",
    "            # Compute current score\n",
    "            nb_current = neighbors[current_idx - 1]\n",
    "            dist_current = nx.shortest_path_length(G, source=node, target=nb_current)\n",
    "            size_current = len(communities[current_idx])\n",
    "            current_score = alpha * dist_current + beta * size_current\n",
    "\n",
    "            best_score = current_score\n",
    "            best_index = current_idx\n",
    "\n",
    "            # Try other communities\n",
    "            for i, nb in enumerate(neighbors, start=1):\n",
    "                if i == current_idx:\n",
    "                    continue\n",
    "                dist = nx.shortest_path_length(G, source=node, target=nb)\n",
    "                size = len(communities[i])\n",
    "                score = alpha * dist + beta * size\n",
    "                if score < best_score:\n",
    "                    best_score = score\n",
    "                    best_index = i\n",
    "\n",
    "            # Move if better community is found\n",
    "            if best_index != current_idx:\n",
    "                communities[current_idx].remove(node)\n",
    "                communities[best_index].add(node)\n",
    "                moved_count += 1\n",
    "\n",
    "        # If no node moved, stop\n",
    "        if moved_count == 0:\n",
    "            break\n",
    "\n",
    "    # ============ 4) Generate all valid key node combinations ============\n",
    "    all_key_nodes_combos = []\n",
    "\n",
    "    if all(len(communities[i]) > 0 for i in range(1, num_communities + 1)):\n",
    "        all_products = product(*(communities[i] for i in range(1, num_communities + 1)))\n",
    "        for combo in all_products:\n",
    "            combo_list = [fixed_node] + list(combo)\n",
    "            all_key_nodes_combos.append(combo_list)\n",
    "    else:\n",
    "        all_key_nodes_combos = []\n",
    "\n",
    "    # ============ 5) Draw graph with community coloring ============\n",
    "    if pos is None:\n",
    "        pos = nx.spring_layout(G, seed=42)\n",
    "\n",
    "    colors = [\"red\", \"blue\", \"green\", \"orange\", \"purple\", \"cyan\", \"yellow\", \"pink\"]\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    for i, community in enumerate(communities):\n",
    "        nx.draw_networkx_nodes(\n",
    "            G, pos, nodelist=community,\n",
    "            node_color=colors[i % len(colors)], \n",
    "            label=f\"Community {i}\",\n",
    "            alpha=0.8,\n",
    "            node_size=100\n",
    "        )\n",
    "\n",
    "    # Highlight the fixed node separately\n",
    "    nx.draw_networkx_nodes(\n",
    "        G, pos, nodelist=[fixed_node],\n",
    "        node_color=\"red\", node_shape=\"o\",\n",
    "        node_size=100, alpha=0.9, label=\"Fixed Node\"\n",
    "    )\n",
    "\n",
    "    nx.draw_networkx_edges(G, pos, edge_color=\"gray\", alpha=0.5)\n",
    "    nx.draw_networkx_labels(G, pos, font_size=8, font_color=\"black\")\n",
    "\n",
    "    plt.legend(\n",
    "        fontsize=6,\n",
    "        borderaxespad=0.5,\n",
    "        labelspacing=0.2,\n",
    "        loc=\"upper left\",\n",
    "        bbox_to_anchor=(1.05, 1),\n",
    "    )\n",
    "    plt.title(\"Graph with Colored Communities (no farthest-node highlight)\")\n",
    "    plt.tight_layout(pad=2.0)\n",
    "\n",
    "    if output_path:\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Graph saved to {output_path}\")\n",
    "    else:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # Return final communities and key node combinations\n",
    "    return communities, all_key_nodes_combos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4249fa-934a-49a8-ab07-f3321bb317ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_er_vs_p(p_range, ER, funcs, cols, output_path, fontsize=12, figsize=(10, 6), dpi=600):\n",
    "    \"\"\"\n",
    "    Plot the relationship between ER and link generation probability p, and save the figure to file.\n",
    "\n",
    "    Args:\n",
    "        p_range (array-like): Range of p values (link generation probabilities).\n",
    "        ER (list of lists): ER data for each protocol function.\n",
    "        funcs (list): List of protocol functions, used to generate legend labels.\n",
    "        cols (list): List of colors corresponding to each function's plot.\n",
    "        output_path (str): File path where the plot will be saved.\n",
    "        fontsize (int, optional): Font size for axis labels and ticks. Default is 12.\n",
    "        figsize (tuple, optional): Figure size in inches. Default is (10, 6).\n",
    "        dpi (int, optional): Resolution (dots per inch) of the output image. Default is 600.\n",
    "    \"\"\"\n",
    "    # Extract function names for legend labels\n",
    "    nom_list = [str(f).split(' ')[1] for f in funcs]\n",
    "\n",
    "    plt.figure(figsize=figsize, dpi=dpi)\n",
    "    plt.grid(linewidth=0.5)\n",
    "\n",
    "    # Plot each protocol's ER values\n",
    "    for i in range(len(funcs)):\n",
    "        y = plt.plot(\n",
    "            p_range, ER[i],\n",
    "            color=cols[i],\n",
    "            marker=\"x\",\n",
    "            linestyle='None',\n",
    "            markersize=3,\n",
    "            # alpha = 0.5,\n",
    "            # linewidth = linewidth,\n",
    "            label=nom_list[i]\n",
    "        )\n",
    "\n",
    "    plt.yscale('log')\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.tick_params(labelsize=fontsize)\n",
    "\n",
    "    plt.xlabel('Link generation probability p', fontsize=fontsize)\n",
    "    plt.ylabel('ER ($\\mathregular{GHZ}_5/\\ \\\\mathregular{T_{slot}}$)', fontsize=fontsize)\n",
    "\n",
    "    ax = plt.gca()\n",
    "    ax.set_xlim([0.2, 1])\n",
    "    ax.set_ylim([0.0001, 1])\n",
    "\n",
    "    # Save the figure\n",
    "    plt.savefig(output_path, dpi=dpi)\n",
    "    plt.close('all')\n",
    "    print(f\"Plot saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f38924d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_p(G_original, combo, p, funcs, timesteps, reps):\n",
    "    \"\"\"\n",
    "    Worker function for evaluating entanglement rate (ER) at a single value of link generation probability p.\n",
    "\n",
    "    This function:\n",
    "        - Creates a deep copy of the original graph to avoid shared state issues in parallel processing.\n",
    "        - Sets edge-level probability `p` across the graph.\n",
    "        - Computes the ER value for each protocol in `funcs` given the same topology and node combination.\n",
    "\n",
    "    Args:\n",
    "        G_original (networkx.Graph): The original graph topology (will be copied internally).\n",
    "        combo (list): A list of key nodes used as inputs to the protocols.\n",
    "        p (float): The link generation probability to apply.\n",
    "        funcs (list): List of protocol functions to evaluate.\n",
    "        timesteps (int): Number of timesteps used per protocol evaluation.\n",
    "        reps (int): Number of repetitions to average over.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of ER values (floats), one per protocol in the same order as `funcs`.\n",
    "    \"\"\"\n",
    "    # Create a deep copy of the graph to avoid shared state issues\n",
    "    G = copy.deepcopy(G_original)\n",
    "\n",
    "    # Set edge-level link generation probability\n",
    "    set_p_edge(G, p_op=p)\n",
    "\n",
    "    # Compute ER for each protocol\n",
    "    p_ers = []\n",
    "    for func in funcs:\n",
    "        er, _, _ = func(G, combo, timesteps=timesteps, reps=reps)\n",
    "        p_ers.append(er)\n",
    "\n",
    "    return p_ers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581ea08b-8032-47f8-be61-a03ee8f072ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== 2) Global Parameters ==============\n",
    "funcs = [MPC_protocol, MPG_protocol, SP_protocol]\n",
    "p_range = np.linspace(1, 0.2, 50)\n",
    "\n",
    "timesteps = 100\n",
    "reps = 200\n",
    "alpha = 1.4\n",
    "beta = 0.105\n",
    "max_rounds = 10\n",
    "shuffle_nodes = True\n",
    "\n",
    "sr_results = []  # SR results for each file will be appended here\n",
    "\n",
    "# Chunked exit: exit after processing a certain number of combinations\n",
    "chunk_size = 300\n",
    "state_file = \"loop_state.pkl\"\n",
    "\n",
    "# ============== 3) Load or Initialize Progress ==============\n",
    "try:\n",
    "    with open(state_file, \"rb\") as f:\n",
    "        progress = pickle.load(f)\n",
    "    print(\"Progress restored:\", progress)\n",
    "except FileNotFoundError:\n",
    "    progress = {\n",
    "        \"subfolder_idx\": 0,        # Index of current subfolder\n",
    "        \"file_idx\": 0,             # Index of current file in subfolder\n",
    "        \"combo_idx\": 0,            # Index of current combination in file\n",
    "        \"global_combo_count\": 0    # Total number of combinations processed\n",
    "    }\n",
    "    print(\"No progress file found, starting from scratch.\")\n",
    "\n",
    "# List of subfolders\n",
    "subfolders = [sf for sf in root_path.iterdir() if sf.is_dir()]\n",
    "subfolders.sort()\n",
    "\n",
    "# ============== 4) Main Loop ==============\n",
    "for s_idx in range(progress[\"subfolder_idx\"], len(subfolders)):\n",
    "    subfolder = subfolders[s_idx]\n",
    "    if not subfolder.is_dir():\n",
    "        continue\n",
    "\n",
    "    print(f\"Processing subfolder: {subfolder}\")\n",
    "\n",
    "    # Iterate over files in subfolder\n",
    "    files = [f for f in subfolder.iterdir() if f.is_file()]\n",
    "    files.sort()\n",
    "\n",
    "    for f_idx in range(progress[\"file_idx\"], len(files)):\n",
    "        file = files[f_idx]\n",
    "        if not file.is_file():\n",
    "            continue\n",
    "\n",
    "        print(f\"  Processing file: {file} ...\")\n",
    "\n",
    "        # Initialize counters for this file\n",
    "        failure_counts = {func.__name__: 0 for func in funcs}\n",
    "        combination_counter = 0\n",
    "\n",
    "        # Load graph\n",
    "        G, users, pos = load_data(file)\n",
    "\n",
    "        G = network(G)\n",
    "\n",
    "        # Output paths\n",
    "        class_folder = subfolder.name\n",
    "        file_path = file.with_suffix(\".png\")\n",
    "        file_name = file_path.name\n",
    "\n",
    "        er_folder_path = Path.cwd().parent.joinpath(\"new_result\", class_folder)\n",
    "        er_folder_path.mkdir(exist_ok=True)\n",
    "        er_topology_folder_path = er_folder_path.joinpath(file_name)\n",
    "        er_topology_folder_path.mkdir(exist_ok=True)\n",
    "        communities_output_path = Path.cwd().parent.joinpath(\"communitie\", class_folder, file_name)\n",
    "\n",
    "        # Run community detection and get combinations\n",
    "        communities, users_node_combination = multi_iterative_score_partition_with_drawing(\n",
    "            G, users[0], alpha, beta, max_rounds, shuffle_nodes, pos, communities_output_path\n",
    "        )\n",
    "\n",
    "        # Randomly sample 100 combinations\n",
    "        sampled_combinations = np.random.choice(\n",
    "            len(users_node_combination),\n",
    "            size=min(100, len(users_node_combination)),\n",
    "            replace=False\n",
    "        )\n",
    "\n",
    "        # Process 100 combinations in this file\n",
    "        for sampled_idx, original_idx in enumerate(sampled_combinations):\n",
    "            combo = users_node_combination[original_idx]\n",
    "            combination_counter += 1\n",
    "            progress[\"global_combo_count\"] += 1\n",
    "\n",
    "            # =========== Combination Metadata ===========\n",
    "            combination_sr = {\n",
    "                \"Combination_ID\": f\"combo_{sampled_idx}\",\n",
    "                \"Nodes\": str(combo),  # Convert list of nodes to string, e.g., \"[2,5,9]\"\n",
    "            }\n",
    "\n",
    "            # Compute ER matrix\n",
    "            ER = np.zeros((len(funcs), len(p_range)))\n",
    "            results = Parallel(n_jobs=-1, verbose=10)(\n",
    "                delayed(process_single_p)(G, combo, p, funcs, timesteps, reps)\n",
    "                for p in p_range\n",
    "            )\n",
    "\n",
    "            # Fill ER matrix\n",
    "            for i, p_ers in enumerate(results):\n",
    "                ER[:, i] = p_ers\n",
    "\n",
    "            plot_er_vs_p(p_range, ER, funcs, cols, er_topology_folder_path.joinpath(f'result_for_{str(combo)}'))\n",
    "\n",
    "            # =========== Calculate Success Ratios ===========\n",
    "            for func_idx, func in enumerate(funcs):\n",
    "                protocol_er = ER[func_idx, :]\n",
    "                zero_count = np.sum(protocol_er < 1e-10)\n",
    "                success_ratio = 1 - (zero_count / len(p_range))\n",
    "                combination_sr[func.__name__] = round(success_ratio, 3)\n",
    "            del results, ER\n",
    "            gc.collect()\n",
    "\n",
    "            # Chunked exit: exit every `chunk_size` combinations\n",
    "            if progress[\"global_combo_count\"] % chunk_size == 0:\n",
    "                print(f\"Processed {progress['global_combo_count']} combinations, exiting for checkpoint.\")\n",
    "                progress[\"subfolder_idx\"] = s_idx\n",
    "                progress[\"file_idx\"] = f_idx\n",
    "                progress[\"combo_idx\"] = sampled_idx + 1  # Resume from next combination\n",
    "                with open(state_file, \"wb\") as pf:\n",
    "                    pickle.dump(progress, pf)\n",
    "                exit()\n",
    "\n",
    "            # Update progress after each combination\n",
    "            progress[\"combo_idx\"] = sampled_idx + 1\n",
    "            progress[\"subfolder_idx\"] = s_idx\n",
    "            progress[\"file_idx\"] = f_idx\n",
    "            with open(state_file, \"wb\") as pf:\n",
    "                pickle.dump(progress, pf)\n",
    "\n",
    "            # =========== Write to CSV ===========\n",
    "            output_subfolder_csv_path = er_topology_folder_path.joinpath(f\"{file.stem}_sr_details.csv\")\n",
    "\n",
    "            # Define headers (written only on first write)\n",
    "            fieldnames = [\"Combination_ID\", \"Nodes\"] + [func.__name__ for func in funcs]\n",
    "\n",
    "            write_header = not output_subfolder_csv_path.exists()\n",
    "\n",
    "            with open(output_subfolder_csv_path, mode=\"a\", newline=\"\") as subfile:\n",
    "                csv_writer = csv.DictWriter(subfile, fieldnames=fieldnames)\n",
    "                if write_header:\n",
    "                    csv_writer.writeheader()\n",
    "                csv_writer.writerow(combination_sr)\n",
    "\n",
    "        # =========== File-Level SR Summary ===========\n",
    "        sr_for_protocols = {}\n",
    "        if combination_counter > 0:\n",
    "            for protocol_name, failures in failure_counts.items():\n",
    "                sr_for_protocols[protocol_name] = (combination_counter - failures) / combination_counter\n",
    "        else:\n",
    "            for protocol_name in failure_counts:\n",
    "                sr_for_protocols[protocol_name] = 0\n",
    "\n",
    "        # Add metadata (subfolder and file name)\n",
    "        sr_for_protocols[\"Subfolder\"] = subfolder.name\n",
    "        sr_for_protocols[\"File\"] = file.name\n",
    "        sr_results.append(sr_for_protocols)\n",
    "\n",
    "        # File complete => reset combo_idx and advance file_idx\n",
    "        progress[\"combo_idx\"] = 0\n",
    "        progress[\"file_idx\"] = f_idx + 1\n",
    "        with open(state_file, \"wb\") as pf:\n",
    "            pickle.dump(progress, pf)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
