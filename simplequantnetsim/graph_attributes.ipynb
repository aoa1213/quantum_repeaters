{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from scipy.linalg import eigh\n",
    "\n",
    "from networkx.algorithms.community import greedy_modularity_communities\n",
    "from networkx.drawing.layout import *\n",
    "from graph import network\n",
    "\n",
    "# 获取当前 Notebook 的绝对路径\n",
    "notebook_path = os.path.abspath(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"..\")  # 确保根目录在 Python 路径中\n",
    "root_path = Path.cwd().parent.joinpath(\"graphs_json\")\n",
    "results_csv = Path.cwd().parent.joinpath(\"new_result\",\"anomaly_results.csv\")\n",
    "excel_file = Path.cwd().parent.joinpath(\"new_result\", \"results.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filepath):\n",
    "    pos = {}\n",
    "    user = []\n",
    "\n",
    "    # Step 1: 读取 JSON 文件\n",
    "    with open(filepath, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Step 2: 初始化图\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Step 3: 添加节点\n",
    "    for node in data[\"nodes\"]:\n",
    "        node_id = node[\"id\"]\n",
    "        x, y = node[\"latitude\"], node[\"longitude\"]\n",
    "        G.add_node(node_id, location=node[\"location\"], country=node[\"country\"])  # 添加节点到图\n",
    "        pos[node_id] = (y, x)  # 保存节点位置，注意 (longitude, latitude)\n",
    "\n",
    "    # Step 4: 添加边\n",
    "    for edge in data[\"links\"]:\n",
    "        source = int(edge[\"source\"])\n",
    "        target = int(edge[\"target\"])\n",
    "        G.add_edge(source, target, length=edge[\"length\"])  # 添加边到图\n",
    "\n",
    "    degree_dict = dict(G.degree())\n",
    "    degree_items = list(degree_dict.items())\n",
    "    first_node,first_degree = degree_items[0]\n",
    "    #print(f\"First node ID: {first_node}, Degree: {first_degree}\")\n",
    "\n",
    "    user.append(data[\"nodes\"][0][\"id\"])\n",
    "\n",
    "    return G,user,pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_anomaly(mpc, mpg, sp):\n",
    "    return mpc > mpg > sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_topology_features(G):\n",
    "    features = {}\n",
    "    \n",
    "    # Basic metrics\n",
    "    features[\"n\"] = G.number_of_nodes()\n",
    "    features[\"m\"] = G.number_of_edges()\n",
    "    features[\"avg_degree\"] = 2*features[\"m\"]/features[\"n\"] if features[\"n\"] > 0 else 0\n",
    "    \n",
    "    # Connectivity check\n",
    "    is_connected = nx.is_connected(G)\n",
    "    \n",
    "    # Diameter (处理非连通图)\n",
    "    if is_connected:\n",
    "        features[\"diameter\"] = nx.diameter(G)\n",
    "    else:\n",
    "        features[\"diameter\"] = max([nx.diameter(c) for c in nx.connected_components(G)], default=0)\n",
    "    \n",
    "    # Network density\n",
    "    features[\"density\"] = nx.density(G)\n",
    "    \n",
    "    # Average shortest path length (仅连通图)\n",
    "    features[\"avg_shortest_path\"] = nx.average_shortest_path_length(G) if is_connected else float('inf')\n",
    "    \n",
    "    # Clustering coefficients\n",
    "    clustering = nx.clustering(G)\n",
    "    features[\"avg_clustering\"] = sum(clustering.values())/len(clustering)\n",
    "    \n",
    "    # Betweenness centrality\n",
    "    edge_betweenness = nx.edge_betweenness_centrality(G)\n",
    "    features[\"max_edge_betweenness\"] = max(edge_betweenness.values(), default=0)\n",
    "    \n",
    "    node_betweenness = nx.betweenness_centrality(G)\n",
    "    features[\"max_node_betweenness\"] = max(node_betweenness.values(), default=0)\n",
    "    \n",
    "    # Global efficiency\n",
    "    shortest_paths = dict(nx.shortest_path_length(G))\n",
    "    efficiency = []\n",
    "    for u in G:\n",
    "        for v in G:\n",
    "            if u != v:\n",
    "                try:\n",
    "                    efficiency.append(1/shortest_paths[u][v])\n",
    "                except KeyError:\n",
    "                    pass\n",
    "    features[\"global_efficiency\"] = sum(efficiency)/(features[\"n\"]*(features[\"n\"]-1)) if features[\"n\"] > 1 else 0\n",
    "    \n",
    "    # Spectral features\n",
    "    A = nx.adjacency_matrix(G).todense()\n",
    "    eigenvalues = np.linalg.eigvals(A)\n",
    "    features[\"spectral_radius\"] = np.max(np.abs(eigenvalues))\n",
    "    \n",
    "    L = nx.normalized_laplacian_matrix(G).todense()\n",
    "    eigenvalues_L = np.linalg.eigvals(L)\n",
    "    features[\"algebraic_connectivity\"] = sorted(eigenvalues_L)[1] if features[\"n\"] >= 2 else 0\n",
    "    \n",
    "    # Weighted Spectral Distribution (WSD)\n",
    "    features[\"WSD\"] = sum((1 - eigenvalues_L)**4)\n",
    "    \n",
    "    # K-connectivity\n",
    "    features[\"node_connectivity\"] = nx.node_connectivity(G) if is_connected else 0\n",
    "    features[\"edge_connectivity\"] = nx.edge_connectivity(G) if is_connected else 0\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {}\n",
    "subfolders = [sf for sf in root_path.iterdir() if sf.is_dir()]\n",
    "subfolders.sort()\n",
    "\n",
    "column_names = []\n",
    "\n",
    "for s_idx in range(len(subfolders)):\n",
    "    subfolder = subfolders[s_idx]\n",
    "    if not subfolder.is_dir():\n",
    "        continue\n",
    "\n",
    "    files = [f for f in subfolder.iterdir() if f.is_file()]\n",
    "    files.sort()\n",
    "\n",
    "    for f_idx in range(len(files)):\n",
    "        anomaly_count = 0\n",
    "        file = files[f_idx]\n",
    "        if not file.is_file():\n",
    "            continue\n",
    "\n",
    "        file_parts = Path(file).parts\n",
    "        the_result_path = Path.cwd().parent.joinpath(\"new_result\", *file_parts[-2:])\n",
    "        new_path = the_result_path.with_suffix('').joinpath(f\"{file.stem}_sr_details.csv\")\n",
    "\n",
    "        try:\n",
    "            with open(new_path, 'r', encoding='utf-8') as f:\n",
    "                reader = csv.reader(f)\n",
    "                headers = next(reader)\n",
    "\n",
    "                for row in reader:\n",
    "                    try:\n",
    "                        mpc, mpg, sp = map(float, row[2:5])\n",
    "                        if not is_anomaly(mpc, mpg, sp):\n",
    "                            anomaly_count += 1\n",
    "                        anomaly_count += 1\n",
    "                        anomaly_percent = anomaly_count/anomaly_count\n",
    "                    except (IndexError, ValueError) as e:\n",
    "                        print(f\"行 {reader.line_num} 数据格式错误: {str(e)}\")\n",
    "                        continue\n",
    "\n",
    "            class_name = new_path.parent.parent.name\n",
    "            relative_path = new_path.relative_to(Path.cwd().parent)\n",
    "\n",
    "            G, _, _ = load_data(file)\n",
    "            features_dict = calculate_topology_features(G)\n",
    "\n",
    "\n",
    "            if not column_names:\n",
    "                column_names = list(features_dict.keys())\n",
    "\n",
    "            values = list(features_dict.values())\n",
    "\n",
    "\n",
    "            row_data = [str(relative_path), str(file), anomaly_percent] + values\n",
    "            results_dict.setdefault(class_name, []).append(row_data)\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"文件 {new_path} 未找到，跳过\")\n",
    "        except Exception as e:\n",
    "            print(f\"处理 {file.name} 失败: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "if results_dict:\n",
    "    with pd.ExcelWriter(excel_file) as writer:\n",
    "        for class_name, rows in results_dict.items():\n",
    "            df = pd.DataFrame(rows, columns=[\"File_Path\", \"Graph_path\", \"anomaly_percent\"] + column_names)\n",
    "            df.to_excel(writer, sheet_name=class_name, index=False)\n",
    "else:\n",
    "    print(\"没有数据可以写入\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
