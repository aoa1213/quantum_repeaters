{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from scipy.linalg import eigh\n",
    "\n",
    "from networkx.algorithms.community import greedy_modularity_communities\n",
    "from networkx.drawing.layout import *\n",
    "from graph import network\n",
    "\n",
    "# 获取当前 Notebook 的绝对路径\n",
    "notebook_path = os.path.abspath(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"..\")  # 确保根目录在 Python 路径中\n",
    "root_path = Path.cwd().parent.joinpath(\"graphs_json\")\n",
    "results_csv = Path.cwd().parent.joinpath(\"new_result\",\"anomaly_results.csv\")\n",
    "excel_file = Path.cwd().parent.joinpath(\"new_result\", \"SR_results.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filepath):\n",
    "    pos = {}\n",
    "    user = []\n",
    "\n",
    "    # Step 1: 读取 JSON 文件\n",
    "    with open(filepath, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Step 2: 初始化图\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Step 3: 添加节点\n",
    "    for node in data[\"nodes\"]:\n",
    "        node_id = node[\"id\"]\n",
    "        x, y = node[\"latitude\"], node[\"longitude\"]\n",
    "        G.add_node(node_id, location=node[\"location\"], country=node[\"country\"])  # 添加节点到图\n",
    "        pos[node_id] = (y, x)  # 保存节点位置，注意 (longitude, latitude)\n",
    "\n",
    "    # Step 4: 添加边\n",
    "    for edge in data[\"links\"]:\n",
    "        source = int(edge[\"source\"])\n",
    "        target = int(edge[\"target\"])\n",
    "        G.add_edge(source, target, length=edge[\"length\"])  # 添加边到图\n",
    "\n",
    "    degree_dict = dict(G.degree())\n",
    "    degree_items = list(degree_dict.items())\n",
    "    first_node,first_degree = degree_items[0]\n",
    "    #print(f\"First node ID: {first_node}, Degree: {first_degree}\")\n",
    "\n",
    "    user.append(data[\"nodes\"][0][\"id\"])\n",
    "\n",
    "    return G,user,pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_topology_features(G):\n",
    "    features = {}\n",
    "    \n",
    "    # Basic metrics\n",
    "    features[\"n\"] = G.number_of_nodes()\n",
    "    features[\"m\"] = G.number_of_edges()\n",
    "    features[\"avg_degree\"] = 2*features[\"m\"]/features[\"n\"] if features[\"n\"] > 0 else 0\n",
    "    \n",
    "    # Connectivity check\n",
    "    is_connected = nx.is_connected(G)\n",
    "    \n",
    "    # Diameter (处理非连通图)\n",
    "    if is_connected:\n",
    "        features[\"diameter\"] = nx.diameter(G)\n",
    "    else:\n",
    "        features[\"diameter\"] = max([nx.diameter(c) for c in nx.connected_components(G)], default=0)\n",
    "    \n",
    "    # Network density\n",
    "    features[\"density\"] = nx.density(G)\n",
    "    \n",
    "    # Average shortest path length (仅连通图)\n",
    "    features[\"avg_shortest_path\"] = nx.average_shortest_path_length(G) if is_connected else float('inf')\n",
    "    \n",
    "    # Clustering coefficients\n",
    "    clustering = nx.clustering(G)\n",
    "    features[\"avg_clustering\"] = sum(clustering.values())/len(clustering)\n",
    "    \n",
    "    # Betweenness centrality\n",
    "    edge_betweenness = nx.edge_betweenness_centrality(G)\n",
    "    features[\"max_edge_betweenness\"] = max(edge_betweenness.values(), default=0)\n",
    "    \n",
    "    node_betweenness = nx.betweenness_centrality(G)\n",
    "    features[\"max_node_betweenness\"] = max(node_betweenness.values(), default=0)\n",
    "    \n",
    "    # Global efficiency\n",
    "    shortest_paths = dict(nx.shortest_path_length(G))\n",
    "    efficiency = []\n",
    "    for u in G:\n",
    "        for v in G:\n",
    "            if u != v:\n",
    "                try:\n",
    "                    efficiency.append(1/shortest_paths[u][v])\n",
    "                except KeyError:\n",
    "                    pass\n",
    "    features[\"global_efficiency\"] = sum(efficiency)/(features[\"n\"]*(features[\"n\"]-1)) if features[\"n\"] > 1 else 0\n",
    "    \n",
    "    # Spectral features\n",
    "    A = nx.adjacency_matrix(G).todense()\n",
    "    eigenvalues = np.linalg.eigvals(A)\n",
    "    features[\"spectral_radius\"] = np.max(np.abs(eigenvalues))\n",
    "    \n",
    "    L = nx.normalized_laplacian_matrix(G).todense()\n",
    "    eigenvalues_L = np.linalg.eigvals(L)\n",
    "    features[\"algebraic_connectivity\"] = sorted(eigenvalues_L)[1] if features[\"n\"] >= 2 else 0\n",
    "    \n",
    "    # Weighted Spectral Distribution (WSD)\n",
    "    features[\"WSD\"] = sum((1 - eigenvalues_L)**4)\n",
    "    \n",
    "    # K-connectivity\n",
    "    features[\"node_connectivity\"] = nx.node_connectivity(G) if is_connected else 0\n",
    "    features[\"edge_connectivity\"] = nx.edge_connectivity(G) if is_connected else 0\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {}\n",
    "subfolders = [sf for sf in root_path.iterdir() if sf.is_dir()]\n",
    "subfolders.sort()\n",
    "\n",
    "column_names = []\n",
    "\n",
    "for s_idx in range(len(subfolders)):\n",
    "    subfolder = subfolders[s_idx]\n",
    "    if not subfolder.is_dir():\n",
    "        continue\n",
    "\n",
    "    files = [f for f in subfolder.iterdir() if f.is_file()]\n",
    "    files.sort()\n",
    "\n",
    "    for f_idx in range(len(files)):\n",
    "        file = files[f_idx]\n",
    "        combination_count = 0\n",
    "        if not file.is_file():\n",
    "            continue\n",
    "\n",
    "        file_parts = Path(file).parts\n",
    "        the_result_path = Path.cwd().parent.joinpath(\"new_result\", *file_parts[-2:])\n",
    "        new_path = the_result_path.with_suffix('').joinpath(f\"{file.stem}_sr_details.csv\")\n",
    "\n",
    "        try:\n",
    "            with open(new_path, 'r', encoding='utf-8') as f:\n",
    "                reader = csv.reader(f)\n",
    "                headers = next(reader)\n",
    "\n",
    "                required_keys = [\"MPC_protocol\", \"MPG_protocol\", \"SP_protocol\"]\n",
    "                protocol_ers = dict.fromkeys(required_keys, 0)\n",
    "                key_indices = {key: headers.index(key) for key in required_keys if key in headers}\n",
    "                \n",
    "                for row in reader:\n",
    "                    try:\n",
    "                        combination_count += 1\n",
    "                        for key, index in key_indices.items():\n",
    "                            protocol_ers[key] += float(row[index])\n",
    "                    except (IndexError, ValueError) as e:\n",
    "                        print(f\"行 {reader.line_num} 数据格式错误: {str(e)}\")\n",
    "                        continue\n",
    "                if combination_count > 0:\n",
    "                    average_protocol_ers = {key: value / combination_count for key, value in protocol_ers.items()}\n",
    "                    improve_ratio_mpc_mpg = ((average_protocol_ers[\"MPC_protocol\"] - average_protocol_ers[\"MPG_protocol\"]) / average_protocol_ers[\"MPG_protocol\"]) * 100 if average_protocol_ers[\"MPG_protocol\"] != 0 else float(\"inf\")\n",
    "                    improve_ratio_mpc_sp = ((average_protocol_ers[\"MPC_protocol\"] - average_protocol_ers[\"SP_protocol\"]) / average_protocol_ers[\"SP_protocol\"]) * 100 if average_protocol_ers[\"SP_protocol\"] != 0 else float(\"inf\")\n",
    "\n",
    "            class_name = new_path.parent.parent.name\n",
    "\n",
    "            G, _, _ = load_data(file)\n",
    "            features_dict = calculate_topology_features(G)\n",
    "\n",
    "\n",
    "            if not column_names:\n",
    "                column_names = list(features_dict.keys())\n",
    "\n",
    "            values = list(features_dict.values())\n",
    "\n",
    "\n",
    "            row_data = [file.stem, combination_count] + list(average_protocol_ers.values()) + [improve_ratio_mpc_mpg, improve_ratio_mpc_sp]+ values\n",
    "            results_dict.setdefault(class_name, []).append(row_data)\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"文件 {new_path} 未找到，跳过\")\n",
    "        except Exception as e:\n",
    "            print(f\"处理 {file.name} 失败: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "if results_dict:\n",
    "    with pd.ExcelWriter(excel_file) as writer:\n",
    "        for class_name, rows in results_dict.items():\n",
    "            columns = [\"topology_name\", \"combination_count\", \"mpc_avg\", \"mpg_avg\", \"sp_avg\",\n",
    "                       \"improve_ratio_mpc_mpg\", \"improve_ratio_mpc_sp\"] + column_names\n",
    "            df = pd.DataFrame(rows, columns=columns)\n",
    "\n",
    "            # # 计算协议部分平均值\n",
    "            # avg_mpc = df[\"mpc_avg\"].mean()\n",
    "            # avg_mpg = df[\"mpg_avg\"].mean()\n",
    "            # avg_sp = df[\"sp_avg\"].mean()\n",
    "            # avg_improve_ratio_mpc_mpg = df[\"improve_ratio_mpc_mpg\"].mean()\n",
    "            # avg_improve_ratio_mpc_sp = df[\"improve_ratio_mpc_sp\"].mean()\n",
    "\n",
    "            # # 计算拓扑特征的平均值\n",
    "            # avg_topo_features = df[column_names].mean()\n",
    "\n",
    "            # # 拼接整行：名称 + 空 + 5个平均值 + 拓扑特征均值\n",
    "            # avg_row_data = [\"Average\", \"\"] + [avg_mpc, avg_mpg, avg_sp,\n",
    "            #                                   avg_improve_ratio_mpc_mpg, avg_improve_ratio_mpc_sp] + list(avg_topo_features)\n",
    "\n",
    "            # avg_row = pd.DataFrame([avg_row_data], columns=columns)\n",
    "\n",
    "            # df = pd.concat([df, avg_row], ignore_index=True)\n",
    "            df.to_excel(writer, sheet_name=class_name, index=False)\n",
    "else:\n",
    "    print(\"没有数据可以写入\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
