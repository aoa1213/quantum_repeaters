{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "from networkx.algorithms.community import greedy_modularity_communities\n",
    "from networkx.drawing.layout import *\n",
    "notebook_path = os.path.abspath(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"..\")\n",
    "root_path = Path.cwd().parent.joinpath(\"graphs_json\")\n",
    "\n",
    "excel_file = Path.cwd().parent.joinpath(\"new_result\", \"SR_results.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filepath):\n",
    "    \"\"\"\n",
    "    Load a network graph from a JSON file in node-link format.\n",
    "\n",
    "    The JSON file is expected to contain:\n",
    "      - \"nodes\": a list of nodes, each with fields \"id\", \"latitude\", \"longitude\", \"location\", and \"country\"\n",
    "      - \"links\": a list of edges, each with fields \"source\", \"target\", and \"length\"\n",
    "\n",
    "    This function:\n",
    "      - Builds a NetworkX graph with node and edge attributes\n",
    "      - Stores node positions using (longitude, latitude) format\n",
    "      - Collects the ID of the first node (assumed to be the user/root)\n",
    "\n",
    "    Args:\n",
    "        filepath (str or Path): Path to the input JSON file.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - G (networkx.Graph): The constructed graph.\n",
    "            - user (list): A list containing the first node's ID.\n",
    "            - pos (dict): Mapping from node ID to (longitude, latitude) positions.\n",
    "    \"\"\"\n",
    "    pos = {}\n",
    "    user = []\n",
    "\n",
    "    # Step 1: Read JSON file\n",
    "    with open(filepath, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Step 2: Initialize graph\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Step 3: Add nodes\n",
    "    for node in data[\"nodes\"]:\n",
    "        node_id = node[\"id\"]\n",
    "        x, y = node[\"latitude\"], node[\"longitude\"]\n",
    "        G.add_node(node_id, location=node[\"location\"], country=node[\"country\"])  # Add node to the graph\n",
    "        pos[node_id] = (y, x)  # Store node position as (longitude, latitude)\n",
    "\n",
    "    # Step 4: Add edges\n",
    "    for edge in data[\"links\"]:\n",
    "        source = int(edge[\"source\"])\n",
    "        target = int(edge[\"target\"])\n",
    "        G.add_edge(source, target, length=edge[\"length\"])  # Add edge to the graph\n",
    "\n",
    "    degree_dict = dict(G.degree())\n",
    "    degree_items = list(degree_dict.items())\n",
    "    first_node, first_degree = degree_items[0]\n",
    "    # print(f\"First node ID: {first_node}, Degree: {first_degree}\")\n",
    "\n",
    "    user.append(data[\"nodes\"][0][\"id\"])\n",
    "\n",
    "    return G, user, pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "def calculate_topology_features(G):\n",
    "    \"\"\"\n",
    "    Calculate a variety of structural and spectral features from a NetworkX graph.\n",
    "\n",
    "    This function extracts global metrics, connectivity measures, centralities,\n",
    "    clustering, path-based features, and spectral properties from the given graph.\n",
    "\n",
    "    Args:\n",
    "        G (networkx.Graph): The input undirected graph.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary of computed features including:\n",
    "            - n: number of nodes\n",
    "            - m: number of edges\n",
    "            - avg_degree: average node degree\n",
    "            - diameter: graph diameter (or max of components if disconnected)\n",
    "            - density: edge density\n",
    "            - avg_shortest_path: average shortest path length (inf if disconnected)\n",
    "            - avg_clustering: average clustering coefficient\n",
    "            - max_edge_betweenness: maximum edge betweenness centrality\n",
    "            - max_node_betweenness: maximum node betweenness centrality\n",
    "            - global_efficiency: average inverse shortest path length over all pairs\n",
    "            - spectral_radius: largest absolute eigenvalue of adjacency matrix\n",
    "            - algebraic_connectivity: second-smallest eigenvalue of normalized Laplacian\n",
    "            - WSD: weighted spectral distribution (∑(1 - λ)^4)\n",
    "            - node_connectivity: minimum number of nodes to disconnect the graph\n",
    "            - edge_connectivity: minimum number of edges to disconnect the graph\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # Basic metrics\n",
    "    features[\"n\"] = G.number_of_nodes()\n",
    "    features[\"m\"] = G.number_of_edges()\n",
    "    features[\"avg_degree\"] = 2 * features[\"m\"] / features[\"n\"] if features[\"n\"] > 0 else 0\n",
    "    \n",
    "    # Connectivity check\n",
    "    is_connected = nx.is_connected(G)\n",
    "    \n",
    "    # Diameter (handle disconnected graphs)\n",
    "    if is_connected:\n",
    "        features[\"diameter\"] = nx.diameter(G)\n",
    "    else:\n",
    "        features[\"diameter\"] = max([nx.diameter(G.subgraph(c)) for c in nx.connected_components(G)], default=0)\n",
    "    \n",
    "    # Network density\n",
    "    features[\"density\"] = nx.density(G)\n",
    "    \n",
    "    # Average shortest path length (only for connected graphs)\n",
    "    features[\"avg_shortest_path\"] = nx.average_shortest_path_length(G) if is_connected else float('inf')\n",
    "    \n",
    "    # Clustering coefficients\n",
    "    clustering = nx.clustering(G)\n",
    "    features[\"avg_clustering\"] = sum(clustering.values()) / len(clustering)\n",
    "    \n",
    "    # Betweenness centrality\n",
    "    edge_betweenness = nx.edge_betweenness_centrality(G)\n",
    "    features[\"max_edge_betweenness\"] = max(edge_betweenness.values(), default=0)\n",
    "    \n",
    "    node_betweenness = nx.betweenness_centrality(G)\n",
    "    features[\"max_node_betweenness\"] = max(node_betweenness.values(), default=0)\n",
    "    \n",
    "    # Global efficiency\n",
    "    shortest_paths = dict(nx.shortest_path_length(G))\n",
    "    efficiency = []\n",
    "    for u in G:\n",
    "        for v in G:\n",
    "            if u != v:\n",
    "                try:\n",
    "                    efficiency.append(1 / shortest_paths[u][v])\n",
    "                except KeyError:\n",
    "                    pass\n",
    "    features[\"global_efficiency\"] = sum(efficiency) / (features[\"n\"] * (features[\"n\"] - 1)) if features[\"n\"] > 1 else 0\n",
    "    \n",
    "    # Spectral features\n",
    "    A = nx.adjacency_matrix(G).todense()\n",
    "    eigenvalues = np.linalg.eigvals(A)\n",
    "    features[\"spectral_radius\"] = np.max(np.abs(eigenvalues))\n",
    "    \n",
    "    L = nx.normalized_laplacian_matrix(G).todense()\n",
    "    eigenvalues_L = np.linalg.eigvals(L)\n",
    "    features[\"algebraic_connectivity\"] = sorted(eigenvalues_L)[1] if features[\"n\"] >= 2 else 0\n",
    "    \n",
    "    # Weighted Spectral Distribution (WSD)\n",
    "    features[\"WSD\"] = sum((1 - eigenvalues_L) ** 4)\n",
    "    \n",
    "    # K-connectivity\n",
    "    features[\"node_connectivity\"] = nx.node_connectivity(G) if is_connected else 0\n",
    "    features[\"edge_connectivity\"] = nx.edge_connectivity(G) if is_connected else 0\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Aggregate protocol success ratios and topology features from result CSVs and write them to an Excel file.\n",
    "\n",
    "This script:\n",
    "    - Iterates through folders containing protocol evaluation results\n",
    "    - For each file, reads the corresponding *_sr_details.csv containing ER (entanglement rate) data\n",
    "    - Computes average ER for each protocol and the improvement ratios of MPC over MPG and SP\n",
    "    - Loads the associated graph topology and calculates structural features\n",
    "    - Appends a combined summary row for each topology including both protocol performance and graph features\n",
    "    - Outputs all data to a multi-sheet Excel file, one sheet per topology class\n",
    "\n",
    "Expected input:\n",
    "    - Result CSV files: <result_root>/<class>/<topology>/<topology>_sr_details.csv\n",
    "    - Graph JSON files that can be loaded via `load_data`\n",
    "\n",
    "Required functions:\n",
    "    - load_data(file): loads a graph from JSON\n",
    "    - calculate_topology_features(G): computes structural features of the graph\n",
    "\"\"\"\n",
    "\n",
    "results_dict = {}\n",
    "subfolders = [sf for sf in root_path.iterdir() if sf.is_dir()]\n",
    "subfolders.sort()\n",
    "\n",
    "column_names = []\n",
    "\n",
    "for s_idx in range(len(subfolders)):\n",
    "    subfolder = subfolders[s_idx]\n",
    "    if not subfolder.is_dir():\n",
    "        continue\n",
    "\n",
    "    files = [f for f in subfolder.iterdir() if f.is_file()]\n",
    "    files.sort()\n",
    "\n",
    "    for f_idx in range(len(files)):\n",
    "        file = files[f_idx]\n",
    "        combination_count = 0\n",
    "        if not file.is_file():\n",
    "            continue\n",
    "\n",
    "        file_parts = Path(file).parts\n",
    "        the_result_path = Path.cwd().parent.joinpath(\"new_result\", *file_parts[-2:])\n",
    "        new_path = the_result_path.with_suffix('').joinpath(f\"{file.stem}_sr_details.csv\")\n",
    "\n",
    "        try:\n",
    "            with open(new_path, 'r', encoding='utf-8') as f:\n",
    "                reader = csv.reader(f)\n",
    "                headers = next(reader)\n",
    "\n",
    "                required_keys = [\"MPC_protocol\", \"MPG_protocol\", \"SP_protocol\"]\n",
    "                protocol_ers = dict.fromkeys(required_keys, 0)\n",
    "                key_indices = {key: headers.index(key) for key in required_keys if key in headers}\n",
    "                \n",
    "                for row in reader:\n",
    "                    try:\n",
    "                        combination_count += 1\n",
    "                        for key, index in key_indices.items():\n",
    "                            protocol_ers[key] += float(row[index])\n",
    "                    except (IndexError, ValueError) as e:\n",
    "                        print(f\"Row {reader.line_num} formatting error: {str(e)}\")\n",
    "                        continue\n",
    "\n",
    "                if combination_count > 0:\n",
    "                    average_protocol_ers = {\n",
    "                        key: value / combination_count\n",
    "                        for key, value in protocol_ers.items()\n",
    "                    }\n",
    "                    improve_ratio_mpc_mpg = (\n",
    "                        (average_protocol_ers[\"MPC_protocol\"] - average_protocol_ers[\"MPG_protocol\"])\n",
    "                        / average_protocol_ers[\"MPG_protocol\"]\n",
    "                    ) * 100 if average_protocol_ers[\"MPG_protocol\"] != 0 else float(\"inf\")\n",
    "                    improve_ratio_mpc_sp = (\n",
    "                        (average_protocol_ers[\"MPC_protocol\"] - average_protocol_ers[\"SP_protocol\"])\n",
    "                        / average_protocol_ers[\"SP_protocol\"]\n",
    "                    ) * 100 if average_protocol_ers[\"SP_protocol\"] != 0 else float(\"inf\")\n",
    "\n",
    "            class_name = new_path.parent.parent.name\n",
    "\n",
    "            # Load graph and calculate topology features\n",
    "            G, _, _ = load_data(file)\n",
    "            features_dict = calculate_topology_features(G)\n",
    "\n",
    "            # Collect column headers only once\n",
    "            if not column_names:\n",
    "                column_names = list(features_dict.keys())\n",
    "\n",
    "            values = list(features_dict.values())\n",
    "\n",
    "            row_data = [file.stem, combination_count] + list(average_protocol_ers.values()) + [\n",
    "                improve_ratio_mpc_mpg, improve_ratio_mpc_sp\n",
    "            ] + values\n",
    "            results_dict.setdefault(class_name, []).append(row_data)\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File {new_path} not found, skipping.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process {file.name}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "# Write collected results to Excel\n",
    "if results_dict:\n",
    "    with pd.ExcelWriter(excel_file) as writer:\n",
    "        for class_name, rows in results_dict.items():\n",
    "            columns = [\n",
    "                \"topology_name\", \"combination_count\",\n",
    "                \"mpc_avg\", \"mpg_avg\", \"sp_avg\",\n",
    "                \"improve_ratio_mpc_mpg\", \"improve_ratio_mpc_sp\"\n",
    "            ] + column_names\n",
    "\n",
    "            df = pd.DataFrame(rows, columns=columns)\n",
    "\n",
    "            # Compute average values for protocol results\n",
    "            # avg_mpc = df[\"mpc_avg\"].mean()\n",
    "            # avg_mpg = df[\"mpg_avg\"].mean()\n",
    "            # avg_sp = df[\"sp_avg\"].mean()\n",
    "            # avg_improve_ratio_mpc_mpg = df[\"improve_ratio_mpc_mpg\"].mean()\n",
    "            # avg_improve_ratio_mpc_sp = df[\"improve_ratio_mpc_sp\"].mean()\n",
    "\n",
    "            # Compute average values for topology features\n",
    "            # avg_topo_features = df[column_names].mean()\n",
    "\n",
    "            # Combine: [label, blank, avg_mpc, avg_mpg, avg_sp, ratio1, ratio2, ...feature averages]\n",
    "            # avg_row_data = [\"Average\", \"\"] + [avg_mpc, avg_mpg, avg_sp,\n",
    "            #                                   avg_improve_ratio_mpc_mpg, avg_improve_ratio_mpc_sp] + list(avg_topo_features)\n",
    "\n",
    "            # avg_row = pd.DataFrame([avg_row_data], columns=columns)\n",
    "            # df = pd.concat([df, avg_row], ignore_index=True)\n",
    "            df.to_excel(writer, sheet_name=class_name, index=False)\n",
    "else:\n",
    "    print(\"No data available to write.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
